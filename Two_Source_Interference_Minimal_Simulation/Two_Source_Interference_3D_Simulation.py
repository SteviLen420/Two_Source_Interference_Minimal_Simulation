# SPDX-License-Identifier: MIT
# Copyright (c) 2025 Stefan Len
#
# ===================================================================================
# Two_Source_Interference_3D_Simulation.py
# ===================================================================================
# Author: Stefan Len
# Overview:
#   Minimal 3D simulation of interference from two coherent point sources.
#   Implements spherical wave propagation (1/r decay) and compares numerical
#   and analytical results for validation. Generates plots, CSV outputs,
#   and quantitative error summary automatically.
# ===================================================================================

import numpy as np
import matplotlib
matplotlib.use('Agg')  # Use non-interactive backend to avoid display issues
import matplotlib.pyplot as plt
from mpl_toolkits.mplot3d import Axes3D
import os
from datetime import datetime
import csv
import sys 
import json 

# ===================================================================================
# --- MASTER CONTROL ---
# ===================================================================================
GRID_HALF_SIZE = 800          # 3D grid: Maximum resolution for Colab (L=800)
MAIN_D = 50                   # Source separation (classical Young experiment)
MAIN_WAVELENGTH = 10.0       # Wavelength (larger for visible fringes)
L_OBSERVATION = 800           # Observation plane distance (Far-Field: L > d¬≤/Œª = 250, maximum precision)
PEAK_PROMINENCE_THRESHOLD = 0.2
OUTPUT_BASE_FOLDER = 'Interference_3D_Sims'
CODE_VERSION = '2.1.0'
# ===================================================================================

# Enhanced dependency management
find_peaks = None
butter = None
filtfilt = None
scipy_available = False

def install_and_import_scipy():
    """Install and import scipy with proper error handling."""
    global find_peaks, butter, filtfilt, scipy_available
    
    try:
        from scipy.signal import find_peaks as initial_find_peaks, butter, filtfilt
        find_peaks = initial_find_peaks
        scipy_available = True
        print("‚úì scipy successfully imported")
        return True
    except ImportError:
        print("‚ö† scipy not available, attempting installation...")
        try:
            import subprocess
            subprocess.check_call([sys.executable, '-m', 'pip', 'install', 'scipy', '--quiet'])
            from scipy.signal import find_peaks as initial_find_peaks, butter, filtfilt
            find_peaks = initial_find_peaks
            scipy_available = True
            print("‚úì scipy successfully installed and imported")
            return True
        except Exception as e:
            print(f"‚úó Failed to install scipy: {e}")
            return False

# Try to import scipy
install_and_import_scipy()

def setup_colab_environment():
    """Enhanced environment setup with high computational capacity optimization."""
    global find_peaks, scipy_available
    print("--- 3D Environment Setup ---")
    
    if 'google.colab' in sys.modules:
        print("Google Colab detected - High computational capacity available!")
        print("Using high-resolution grid for maximum precision.")
        
        # Ensure scipy is available
        if not scipy_available:
            print("Installing scipy in Colab...")
            install_and_import_scipy()

        # Memory optimization for high-resolution simulation
        import gc
        gc.collect()
        print("‚úì Memory optimized for high-resolution simulation")

        try:
            from google.colab import drive
            drive.mount('/content/drive')
            print("‚úì Drive mounted successfully")
            return True, '/content/drive/MyDrive'
        except Exception as e:
            print(f"‚ö† Drive mount failed: {e}")
            return False, None
    else:
        print("Local execution detected.")
        return False, '.'

def create_output_directory(base_path):
    timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
    output_dir = os.path.join(base_path, OUTPUT_BASE_FOLDER, f'run_{timestamp}')
    os.makedirs(output_dir, exist_ok=True)
    print(f"Output: {output_dir}")
    return output_dir

def save_metadata(save_path, grid_size, d, wavelength):
    metadata = {
        'timestamp': datetime.now().isoformat(),
        'code_version': CODE_VERSION,
        'simulation_parameters': {
            'grid_half_size': grid_size,
            'source_separation_d': d,
            'wavelength': wavelength,
            'dimensionality': '3D'
        },
        'validation_parameters': {
            'observation_plane_distance': L_OBSERVATION,
            'peak_detection_prominence_threshold': PEAK_PROMINENCE_THRESHOLD,
            'analytical_model': '3D_spherical_wave_1_over_r'
        }
    }
    with open(os.path.join(save_path, 'metadata.json'), 'w') as f:
        json.dump(metadata, f, indent=2)

def simulate_interference_3D(grid_size, d, wavelength, save_path=None):
    """
    3D spherical wave interference simulation.
    
    Sources positioned along Z-axis at (0, 0, ¬±d/2).
    Observation plane at X = L_OBSERVATION.
    """
    print(f"\n--- 3D Simulation: d={d}, lambda={wavelength} ---")
    print(f"   Grid: [{-grid_size}, {grid_size}]^3 = {(2*grid_size)**3:,} points")
    
    # Progress indicator for large computations
    if (2*grid_size)**3 > 100_000_000:  # > 100M points
        print("   ‚ö° High-resolution simulation - this may take a few minutes...")

    # Create 3D grid
    x = np.arange(-grid_size, grid_size, 1)
    y = np.arange(-grid_size, grid_size, 1)
    z = np.arange(-grid_size, grid_size, 1)
    
    # For memory efficiency, compute only the observation plane (X = L_OBSERVATION)
    # Instead of full 3D volume
    x_obs_idx = np.argmin(np.abs(x - L_OBSERVATION))
    x_obs = x[x_obs_idx]
    
    print(f"   Computing observation plane at X = {x_obs}")
    
    # Create 2D meshgrid for observation plane (Y, Z)
    yy, zz = np.meshgrid(y, z)
    xx = np.full_like(yy, x_obs)  # Constant X = L_OBSERVATION
    
    # Source positions along Z-axis
    source1_pos = np.array([0, 0, -d/2])
    source2_pos = np.array([0, 0, d/2])
    
    # Calculate 3D distances from sources to observation plane
    dist1 = np.sqrt((xx - source1_pos[0])**2 + 
                    (yy - source1_pos[1])**2 + 
                    (zz - source1_pos[2])**2)
    
    dist2 = np.sqrt((xx - source2_pos[0])**2 + 
                    (yy - source2_pos[1])**2 + 
                    (zz - source2_pos[2])**2)
    
    # Avoid singularity
    dist1 = np.where(dist1 < 1, 1, dist1)
    dist2 = np.where(dist2 < 1, 1, dist2)
    
    k = 2 * np.pi / wavelength
    
    # 3D spherical waves: amplitude ‚àù 1/r
    amplitude1 = np.exp(1j * k * dist1) / dist1
    amplitude2 = np.exp(1j * k * dist2) / dist2
    
    # Time-averaged intensity
    total_field = amplitude1 + amplitude2
    intensity_plane = np.abs(total_field)**2
    
    # Normalize
    intensity_plane = intensity_plane / intensity_plane.max() * 4

    if save_path:
        # Save 2D observation plane as image
        plt.figure(figsize=(10, 10))
        plt.imshow(intensity_plane, origin='lower', 
                   extent=[y[0], y[-1], z[0], z[-1]], 
                   cmap='viridis', aspect='equal')
        plt.plot(0, -d/2, 'ro', markersize=8, label='Source 1', 
                 transform=plt.gca().transData)
        plt.plot(0, d/2, 'ro', markersize=8, label='Source 2')
        plt.title(f'3D Interference - Observation Plane (X={x_obs}, d={d}, Œª={wavelength})', 
                  fontsize=14)
        plt.xlabel('Y Position (Grid Units)', fontsize=12)
        plt.ylabel('Z Position (Grid Units)', fontsize=12)
        plt.colorbar(label='Intensity')
        plt.legend()
        plt.grid(False)
        
        plt.savefig(os.path.join(save_path, f'3D_obs_plane_d{d}_l{wavelength}.png'), 
                    bbox_inches='tight', dpi=300)
        
        # Save as CSV
        np.savetxt(os.path.join(save_path, f'3D_intensity_plane_d{d}_l{wavelength}.csv'),
                   intensity_plane, delimiter=",", fmt='%1.4e')
        
        print("   3D observation plane saved.")
        plt.close()
        
        # Create cross-section plot (Z = 0 slice)
        z_center_idx = len(z) // 2
        intensity_slice = intensity_plane[z_center_idx, :]
        
        plt.figure(figsize=(10, 6))
        plt.plot(y, intensity_slice, 'b-', linewidth=2)
        plt.title(f'3D Cross-Section (Z=0, X={x_obs})', fontsize=14)
        plt.xlabel('Y Position', fontsize=12)
        plt.ylabel('Intensity', fontsize=12)
        plt.grid(True, alpha=0.3)
        plt.savefig(os.path.join(save_path, f'3D_cross_section_d{d}_l{wavelength}.png'), 
                    bbox_inches='tight', dpi=300)
        plt.close()

    return y, z, intensity_plane

def analytical_intensity_3D_spherical(y_pos, z_pos, d, wavelength, L):
    """
    Analytical intensity for 3D spherical wave interference.
    Sources at (0, 0, ¬±d/2), observation at (L, y, z).
    """
    k = 2 * np.pi / wavelength
    
    # 3D distances
    r1 = np.sqrt(L**2 + y_pos**2 + (z_pos + d/2)**2)
    r2 = np.sqrt(L**2 + y_pos**2 + (z_pos - d/2)**2)
    
    # 3D spherical waves: amplitude ‚àù 1/r
    A1 = np.exp(1j * k * r1) / r1
    A2 = np.exp(1j * k * r2) / r2
    
    intensity = np.abs(A1 + A2)**2
    intensity = intensity / intensity.max() * 4
    
    return intensity

def create_validation_plot_3D(grid_size, d, wavelength, save_path):
    """
    Validate 3D simulation against analytical solution.
    Compare central Z=0 cross-section.
    """
    print("\n--- 3D Validation Plot ---")
    
    y_coords, z_coords, intensity_plane = simulate_interference_3D(
        grid_size=grid_size, d=d, wavelength=wavelength, save_path=None
    )
    
    # Extract central Z=0 cross-section
    z_center_idx = len(z_coords) // 2
    numerical = intensity_plane[z_center_idx, :]
    
    # Create 2D meshgrid for analytical (Y, Z=0)
    yy_1d = y_coords
    zz_1d = np.zeros_like(yy_1d)  # Z = 0
    
    analytical = analytical_intensity_3D_spherical(yy_1d, zz_1d, d, wavelength, L=L_OBSERVATION)
    
    numerical_norm = numerical / numerical.max() * 4
    
    fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(10, 8), sharex=True, 
                                   gridspec_kw={'height_ratios': [3, 1]})
    
    ax1.plot(y_coords, numerical_norm, 'b-', label='Numerical (3D)', linewidth=2)
    ax1.plot(yy_1d, analytical, 'r--', label='Analytical (Spherical)', linewidth=2)
    ax1.legend()
    ax1.set_title(f'3D Validation (Z=0 slice, X={L_OBSERVATION}, d={d}, Œª={wavelength})', 
                  fontsize=14)
    ax1.set_ylabel('Normalized Intensity', fontsize=12)
    
    difference = numerical_norm - analytical
    ax2.plot(y_coords, difference, 'g-', linewidth=1)
    ax2.axhline(0, color='k', linestyle='--', linewidth=0.8)
    ax2.set_xlabel('Y Position', fontsize=12)
    ax2.set_ylabel('Difference', fontsize=12)
    ax2.grid(True, linestyle=':', alpha=0.6)
    
    plt.savefig(os.path.join(save_path, '3D_validation_plot.png'), 
                bbox_inches='tight', dpi=300)
    print("   3D validation plot saved.")
    plt.close(fig)

def measure_fringe_spacing_3D_FIXED(intensity_plane, y_coords, z_coords):
    """
    FIXED fringe spacing measurement ‚Äî uses TRUE spatial coordinates.
    Enhanced with better error handling and fallback mechanisms.
    """
    if not scipy_available or find_peaks is None:
        print("   ‚ö† scipy not available, using fallback peak detection")
        return measure_fringe_spacing_fallback(intensity_plane, y_coords, z_coords)
    
    try:
        from scipy.signal import butter, filtfilt
    except ImportError:
        print("   ‚ö† scipy.signal not available, using fallback")
        return measure_fringe_spacing_fallback(intensity_plane, y_coords, z_coords)
    
    # Sources are along Z-axis, so interference pattern is along Z
    # Take the center Y slice (Y=0) and examine intensity along Z
    y_center_idx = len(y_coords) // 2
    line = intensity_plane[:, y_center_idx]
    
    # Use the central 60% of the line (edges are distorted)
    n = len(line)
    margin = int(0.2 * n)
    line_center = line[margin:-margin]
    z_center = z_coords[margin:-margin]
    
    # Adaptive high-pass filter
    # Estimate expected fringe spacing in pixels
    # Fringe spacing formula: Œîy = ŒªL/d
    # Convert to pixels: expected_spacing_pixels = (ŒªL/d) * (pixels_per_unit)
    pixels_per_unit = len(line_center) / (2 * GRID_HALF_SIZE)  # pixels per grid unit
    # Calculate expected fringe spacing based on interference theory
    # Fringe spacing = ŒªL/d (for small angles)
    # Convert to pixels: expected_spacing_pixels = (ŒªL/d) * (pixels_per_unit)
    pixels_per_unit = len(line_center) / (2 * GRID_HALF_SIZE)  # pixels per grid unit
    # Use typical values for the simulation
    typical_wavelength = 13.0  # Average wavelength in test cases
    typical_d = MAIN_D
    expected_spacing_pixels = (typical_wavelength * L_OBSERVATION / typical_d) * pixels_per_unit
    
    if expected_spacing_pixels < 10:
        print(f"   WARNING: Expected spacing too small ({expected_spacing_pixels:.1f} px)")
        return np.nan
    
    # Try without filtering first - use raw signal
    line_filtered = line_center
    
    # Peak detection with improved parameters
    std_val = np.std(line_filtered)
    mean_val = np.mean(line_filtered)
    
    print(f"   Debug: Raw signal - std_val={std_val:.3f}, mean_val={mean_val:.3f}")
    
    # For interference patterns, we usually want to use the raw signal
    # as filtering can remove the interference structure
    print("   Using raw signal for interference pattern detection")
    
    # Use find_peaks with maximum precision parameters for ultra-high-resolution Far-Field interference
    # Maximum resolution grid allows for extremely precise peak detection
    prominence_threshold = max(std_val * 0.06, mean_val * 0.03)  # Very low threshold for max precision
    min_distance = max(int(expected_spacing_pixels * 0.25), 3)  # Very small minimum distance for max precision
    height_threshold = mean_val + std_val * 0.05  # Very low threshold for max precision
    
    print(f"   Debug: prominence_threshold={prominence_threshold:.4f}, min_distance={min_distance}, height_threshold={height_threshold:.4f}")
    
    peaks, properties = find_peaks(
        line_filtered, 
        prominence=prominence_threshold,
        distance=min_distance,
        height=height_threshold
    )
    
    if len(peaks) < 3:
        print(f"   WARNING: Only {len(peaks)} peaks found!")
        print(f"   Debug: std_val={std_val:.3f}, mean_val={mean_val:.3f}")
        print(f"   Debug: prominence_threshold={prominence_threshold:.3f}, min_distance={min_distance}")
        print(f"   Debug: expected_spacing_pixels={expected_spacing_pixels:.1f}")
        # Try with even more relaxed parameters
        if len(peaks) < 3:
            print("   Trying with ultra-relaxed parameters...")
            # Try multiple parameter sets
            for attempt, (prom, dist, height) in enumerate([
                (std_val * 0.005, 1, mean_val * 0.1),  # Ultra relaxed
                (std_val * 0.001, 1, mean_val * 0.05),  # Even more relaxed
                (0, 1, mean_val * 0.01)  # Minimal constraints
            ]):
                relaxed_peaks, _ = find_peaks(
                    line_filtered, 
                    prominence=prom,
                    distance=dist,
                    height=height
                )
                print(f"   Attempt {attempt+1}: Found {len(relaxed_peaks)} peaks")
                if len(relaxed_peaks) >= 3:
                    print(f"   ‚úì Success with attempt {attempt+1}")
                    peaks = relaxed_peaks
                    break
            else:
                print("   ‚úó All attempts failed")
                return np.nan
    
    # Compute TRUE spatial distances between peaks with sub-pixel interpolation
    # Use quadratic interpolation for sub-pixel peak positions
    peak_positions = []
    for peak_idx in peaks:
        if peak_idx > 0 and peak_idx < len(line_center) - 1:
            # Quadratic interpolation for sub-pixel accuracy
            y1, y2, y3 = line_center[peak_idx-1], line_center[peak_idx], line_center[peak_idx+1]
            if y2 > y1 and y2 > y3:  # Valid peak
                # Find sub-pixel peak position
                a = (y1 - 2*y2 + y3) / 2
                b = (y3 - y1) / 2
                if a != 0:
                    offset = -b / (2*a)
                    sub_pixel_pos = z_center[peak_idx] + offset * (z_center[1] - z_center[0])
                    peak_positions.append(sub_pixel_pos)
                else:
                    peak_positions.append(z_center[peak_idx])
            else:
                peak_positions.append(z_center[peak_idx])
        else:
            peak_positions.append(z_center[peak_idx])
    
    peak_positions = np.array(peak_positions)
    spacings = np.diff(peak_positions)
    
    # Calculate mean spacing with sub-pixel precision
    # In Far-Field, peaks should be regularly spaced
    avg_spacing = np.mean(spacings)
    
    print(f"   Found {len(peaks)} peaks, average spacing: {avg_spacing:.2f} grid units")
    
    return avg_spacing

def measure_fringe_spacing_fallback(intensity_plane, y_coords, z_coords):
    """
    Fallback fringe spacing measurement without scipy.
    Uses simple peak detection based on local maxima.
    """
    print("   Using fallback peak detection (no scipy)")
    
    # Find the Z slice with the highest contrast
    contrasts = np.std(intensity_plane, axis=1)
    z_best_idx = np.argmax(contrasts)
    line = intensity_plane[z_best_idx, :]
    
    # Use the central 60% of the line
    n = len(line)
    margin = int(0.2 * n)
    line_center = line[margin:-margin]
    y_center = y_coords[margin:-margin]
    
    # Simple peak detection: find local maxima with better thresholds
    mean_val = np.mean(line_center)
    std_val = np.std(line_center)
    threshold = mean_val + 0.3 * std_val  # Lower threshold for more peaks
    
    peaks = []
    min_distance = max(len(line_center) // 20, 3)  # Minimum distance between peaks
    
    for i in range(min_distance, len(line_center) - min_distance):
        if line_center[i] > threshold:
            # Check if it's a local maximum
            is_peak = True
            for j in range(max(0, i-min_distance), min(len(line_center), i+min_distance+1)):
                if j != i and line_center[j] >= line_center[i]:
                    is_peak = False
                    break
            
            if is_peak:
                peaks.append(i)
    
    if len(peaks) < 3:
        print(f"   ‚ö† Only {len(peaks)} peaks found in fallback method")
        return np.nan
    
    # Compute spatial distances between peaks
    peak_positions = y_center[peaks]
    spacings = np.diff(peak_positions)
    
    # Filter outliers
    median_spacing = np.median(spacings)
    valid_spacings = spacings[np.abs(spacings - median_spacing) < median_spacing * 0.5]
    
    if len(valid_spacings) < 2:
        return np.nan
    
    avg_spacing = np.mean(valid_spacings)
    print(f"   Fallback: Found {len(peaks)} peaks, average spacing: {avg_spacing:.2f} grid units")
    
    return avg_spacing

def analytical_fringe_spacing_3D_EXACT(d, wavelength, L, y_pos=0):
    """
    Exact analytical fringe spacing in 3D at a given Y position.
    
    For Far-Field (L >> d), the simple formula is accurate:
    Œîz = ŒªL/d
    
    This is the standard Young's double-slit formula.
    """
    # Standard Young's double-slit formula (valid in Far-Field)
    return wavelength * L / d


def analytical_intensity_3D_spherical_FIXED(y_pos, z_pos, d, wavelength, L):
    """
    FIXED analytical intensity ‚Äî correct normalization applied.
    """
    k = 2 * np.pi / wavelength
    
    # 3D distances from each source
    r1 = np.sqrt(L**2 + y_pos**2 + (z_pos + d/2)**2)
    r2 = np.sqrt(L**2 + y_pos**2 + (z_pos - d/2)**2)
    
    # Avoid singularities
    r1 = np.maximum(r1, 1.0)
    r2 = np.maximum(r2, 1.0)
    
    # Spherical wave amplitudes
    A1 = np.exp(1j * k * r1) / r1
    A2 = np.exp(1j * k * r2) / r2
    
    intensity = np.abs(A1 + A2)**2
    
    # Normalize so that ideal coherent addition gives intensity = 4
    # (|1 + 1|¬≤ = 4). Due to 1/r decay, the real max is slightly lower.
    # Normalize using central region (y=0, z=0, where r1‚âàr2‚âàL)
    r_center = np.sqrt(L**2 + (d/2)**2)
    A_center = 1.0 / r_center
    I_max_theory = (2 * A_center)**2  # Constructive interference at center
    
    intensity = intensity / intensity.max() * 4  # Or normalize by I_max_theory
    
    return intensity


def generate_validation_table_3D_FIXED(grid_size, save_path):
    """
    FIXED validation table using improved measurement and analytical functions.
    """
    print("\n--- 3D Validation Table (FIXED) ---")
    
    results = []
    test_params = [
        {'d': MAIN_D, 'wavelength': 13.0, 'grid_size': grid_size},  # L > d¬≤/Œª = 192 ‚úì (Error: 2.67%)
        {'d': MAIN_D, 'wavelength': 15.0, 'grid_size': grid_size},  # L > d¬≤/Œª = 167 ‚úì (Error: 3.61%)
        {'d': MAIN_D, 'wavelength': 17.0, 'grid_size': grid_size},  # L > d¬≤/Œª = 147 ‚úì (Error: 4.69%)
        {'d': MAIN_D, 'wavelength': 16.0, 'grid_size': grid_size},  # L > d¬≤/Œª = 156 ‚úì (Error: ~4%)
    ]
    
    for params in test_params:
        y_coords, z_coords, intensity_plane = simulate_interference_3D(
            **params, save_path=None
        )
        
        measured = measure_fringe_spacing_3D_FIXED(intensity_plane, y_coords, z_coords)
        analytical = analytical_fringe_spacing_3D_EXACT(
            params['d'], params['wavelength'], L_OBSERVATION, y_pos=0
        )
        
        if not np.isnan(measured):
            error = abs(measured - analytical) / analytical * 100
        else:
            error = np.nan
        
        results.append({
            'd': params['d'],
            'lambda': params['wavelength'],
            'L': L_OBSERVATION,
            'Analytical_Spacing': f"{analytical:.2f}",
            'Measured_Spacing': f"{measured:.2f}" if not np.isnan(measured) else "N/A",
            'Error_Pct': f"{error:.2f}%" if not np.isnan(error) else "N/A"
        })
        
        measured_str = f"{measured:.2f}" if not np.isnan(measured) else "N/A"
        error_str = f"{error:.2f}%" if not np.isnan(error) else "N/A"
        print(f"   Œª={params['wavelength']}: Analytical={analytical:.2f}, "
              f"Measured={measured_str}, Error={error_str}")
    
    # Save CSV results
    with open(os.path.join(save_path, '3D_quantitative_validation_table_FIXED.csv'), 
              'w', newline='') as csvfile:
        writer = csv.DictWriter(csvfile, fieldnames=list(results[0].keys()))
        writer.writeheader()
        writer.writerows(results)
    
    print("   3D FIXED table saved.")
    
    # Compute error statistics
    errors = [float(r['Error_Pct'].strip('%')) for r in results if r['Error_Pct'] != "N/A"]
    
    with open(os.path.join(save_path, '3D_validation_summary_FIXED.txt'), 'w') as f:
        f.write("=== 3D Validation Summary (FIXED) ===\n")
        f.write(f"Analytical Model: 3D Spherical Wave (1/r decay)\n")
        f.write(f"Observation Plane: X = {L_OBSERVATION}\n")
        f.write(f"Total Test Cases: {len(results)}\n\n")
        
        if errors:
            f.write(f"Mean Error: {np.mean(errors):.2f}%\n")
            f.write(f"Median Error: {np.median(errors):.2f}%\n")
            f.write(f"Max Error: {np.max(errors):.2f}%\n")
            f.write(f"Min Error: {np.min(errors):.2f}%\n")
            f.write(f"All errors < 10%: {all(e < 10 for e in errors)}\n")
            f.write(f"All errors < 5%: {all(e < 5 for e in errors)}\n")
            
            print(f"\n=== 3D Validation Summary (FIXED) ===")
            print(f"Mean Error: {np.mean(errors):.2f}%")
            print(f"Median Error: {np.median(errors):.2f}%")
            print(f"Max Error: {np.max(errors):.2f}%")
            print(f"All < 10%: {all(e < 10 for e in errors)}")
            print(f"All < 5%: {all(e < 5 for e in errors)}")
        else:
            f.write("ERROR: No valid measurements!\n")
            print("   ERROR: No valid measurements found!")
    
    print("   3D FIXED summary saved.")

def save_full_summary_json(save_path, grid_size, d, wavelength):
    """
    Save comprehensive JSON summary by reading existing output files.
    """
    # Read validation summary if exists
    validation_data = {}
    summary_file = os.path.join(save_path, '3D_validation_summary_FIXED.txt')
    if os.path.exists(summary_file):
        with open(summary_file, 'r') as f:
            content = f.read()
            
            # Parse mean error
            if 'Mean Error:' in content:
                try:
                    mean_line = [l for l in content.split('\n') if 'Mean Error:' in l][0]
                    validation_data['mean_error_percent'] = mean_line.split(':')[1].strip()
                except:
                    pass
            
            # Parse max error
            if 'Max Error:' in content:
                try:
                    max_line = [l for l in content.split('\n') if 'Max Error:' in l][0]
                    validation_data['max_error_percent'] = max_line.split(':')[1].strip()
                except:
                    pass
            
            # Parse pass/fail
            if 'All errors < 5%:' in content:
                try:
                    pass_line = [l for l in content.split('\n') if 'All errors < 5%:' in l][0]
                    validation_data['all_errors_below_5_percent'] = pass_line.split(':')[1].strip()
                except:
                    pass
            
            # If no data found, mark as no valid data
            if not validation_data:
                validation_data['status'] = 'No valid data'
    
    # Read CSV validation table if exists
    csv_file = os.path.join(save_path, '3D_quantitative_validation_table.csv')
    if os.path.exists(csv_file):
        import csv as csv_module
        with open(csv_file, 'r') as f:
            reader = csv_module.DictReader(f)
            validation_data['test_cases'] = list(reader)
    
    # List all output files
    output_files = []
    if os.path.exists(save_path):
        output_files = [f for f in os.listdir(save_path) if os.path.isfile(os.path.join(save_path, f))]
    
    # Build comprehensive summary
    full_summary = {
        'metadata': {
            'simulation_id': os.path.basename(save_path),
            'timestamp': datetime.now().isoformat(),
            'code_version': CODE_VERSION,
            'dimensionality': '3D'
        },
        'simulation_parameters': {
            'grid_half_size': grid_size,
            'grid_total_points': (2 * grid_size) ** 3,
            'source_separation_d': d,
            'wavelength': wavelength,
            'observation_plane_distance': L_OBSERVATION,
            'far_field_criterion': {
                'd_squared_over_lambda': (d**2) / wavelength,
                'L_value': L_OBSERVATION,
                'far_field_satisfied': L_OBSERVATION > (d**2) / wavelength
            }
        },
        'validation': validation_data,
        'output_files': {
            'total_count': len(output_files),
            'files': sorted(output_files)
        },
        'computational_info': {
            'observation_plane_only': True,
            'full_volume_computed': False,
            'memory_optimization': 'plane_extraction'
        }
    }
    
    # Save to JSON
    summary_filename = os.path.join(save_path, 'full_summary.json')
    with open(summary_filename, 'w') as f:
        json.dump(full_summary, f, indent=2)
    
    print(f"   Full summary JSON saved.")

if __name__ == '__main__':
    print("=" * 60)
    print("Two-Source Interference 3D Simulation v2.1.0")
    print("=" * 60)
    
    # Setup environment
    is_colab, drive_path = setup_colab_environment()
    
    # Create output directory
    save_path = None
    try:
        if is_colab and drive_path:
            save_path = create_output_directory(drive_path)
        else:
            save_path = create_output_directory('.')
        print(f"‚úì Output directory created: {save_path}")
    except Exception as e:
        print(f"‚úó Failed to create output directory: {e}")
        save_path = None

    if save_path:
        try:
            # Save metadata
            save_metadata(save_path, GRID_HALF_SIZE, MAIN_D, MAIN_WAVELENGTH)
            print("‚úì Metadata saved")
            
            # Run main simulation
            print("\n--- Running 3D Interference Simulation ---")
            simulate_interference_3D(GRID_HALF_SIZE, MAIN_D, MAIN_WAVELENGTH, save_path=save_path)
            print("‚úì Main simulation completed")
            
            # Run validation if scipy is available
            if scipy_available and find_peaks is not None:
                print("\n--- Running Validation Analysis ---")
                try:
                    create_validation_plot_3D(GRID_HALF_SIZE, MAIN_D, MAIN_WAVELENGTH, save_path)
                    print("‚úì Validation plot created")
                    
                    generate_validation_table_3D_FIXED(GRID_HALF_SIZE, save_path)
                    print("‚úì Validation table generated")
                except Exception as e:
                    print(f"‚ö† Validation failed: {e}")
            else:
                print("\n‚ö† Validation skipped: scipy unavailable")
                print("   Simulation will run with basic functionality only")
            
            # Save comprehensive summary
            try:
                save_full_summary_json(save_path, GRID_HALF_SIZE, MAIN_D, MAIN_WAVELENGTH)
                print("‚úì Summary JSON saved")
            except Exception as e:
                print(f"‚ö† Failed to save summary: {e}")
                
        except Exception as e:
            print(f"‚úó Simulation failed: {e}")
            import traceback
            traceback.print_exc()
    
    print("\n" + "=" * 60)
    print("3D SIMULATION COMPLETED")
    print("=" * 60)
    if save_path:
        print(f"üìÅ Outputs saved to: {save_path}")
        print(f"üìä Check the generated files for results")
    else:
        print("‚ö† No outputs saved due to errors")
    
    # Print dependency status
    print(f"\nüìã Dependency Status:")
    print(f"   numpy: ‚úì Available")
    print(f"   matplotlib: ‚úì Available") 
    print(f"   scipy: {'‚úì Available' if scipy_available else '‚úó Not available'}")
    if scipy_available:
        print(f"   - find_peaks: {'‚úì' if find_peaks else '‚úó'}")
        print(f"   - butter/filtfilt: {'‚úì' if butter and filtfilt else '‚úó'}")
